{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supermario.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxQfVbk5Q1N6J5fBkO6onk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzmtsvv/RL-with-gym/blob/main/supermario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcIAd27FZbuq",
        "outputId": "fe63b794-abcc-4966-c55e-ea4a9797bf15"
      },
      "source": [
        "! pip install gym-super-mario-bros==7.3.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros==7.3.0 in /usr/local/lib/python3.7/dist-packages (7.3.0)\n",
            "Requirement already satisfied: nes-py>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros==7.3.0) (8.1.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.61.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5G6i45l-oQg"
      },
      "source": [
        "'''\n",
        "Implementation of DDQN (CNN) via arxiv.org/pdf/1509.06461.pdf\n",
        "on super mario bros environment using pytorch\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hu4Lq9KaA3-"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
        "from gym.spaces import Box\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from skimage import transform"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRI51rAbapgw",
        "outputId": "60422199-23ad-4686-f027-e12b482176f0"
      },
      "source": [
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "\n",
        "COMPLEX_MOVEMENT = [\n",
        "    ['NOOP'],\n",
        "    ['right'],\n",
        "    ['right', 'A'],\n",
        "    ['right', 'B'],\n",
        "    ['right', 'A', 'B'],\n",
        "    ['A'],\n",
        "    ['left'],\n",
        "    ['left', 'A'],\n",
        "    ['left', 'B'],\n",
        "    ['left', 'A', 'B'],\n",
        "    ['down'],\n",
        "    ['up'],\n",
        "]\n",
        "\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9NPFcCIbJeS"
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "  def __init__(self, env, skip):\n",
        "    \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "    super().__init__(env)\n",
        "    self._skip = skip\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Repeat action, and sum reward\"\"\"\n",
        "    total_reward = 0.0\n",
        "    done = False\n",
        "    for i in range(self._skip):\n",
        "      # Accumulate reward and repeat the same action\n",
        "      obs, reward, done, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env, shape):\n",
        "    super().__init__(env)\n",
        "    if isinstance(shape, int):\n",
        "      self.shape = (shape, shape)\n",
        "    else:\n",
        "      self.shape = tuple(shape)\n",
        "\n",
        "    obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def observation(self, observation):\n",
        "    resize_obs = transform.resize(observation, self.shape)\n",
        "    # cast float back to uint8\n",
        "    resize_obs *= 255\n",
        "    resize_obs = resize_obs.astype(np.uint8)\n",
        "    return resize_obs\n",
        "\n",
        "\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env, keep_dim=False)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = TransformObservation(env, f=lambda x: x / 255.)\n",
        "env = FrameStack(env, num_stack=4)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF-8FlAzbY0C"
      },
      "source": [
        "class ArseNet(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    c, h, w = input_dim\n",
        "    if h != 84:\n",
        "      raise ArithmeticError(f\"Expected input height: 84, got: {h}\")\n",
        "    if w != 84:\n",
        "      raise ArithmeticError(f\"Expected input width: 84, got: {w}\")\n",
        "\n",
        "    self.online = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, output_dim)\n",
        "    )\n",
        "    self.target = copy.deepcopy(self.online)\n",
        "\n",
        "    for theta in self.target.parameters():\n",
        "      theta.requires_grad = False\n",
        "  \n",
        "  def forward(self, inputs, model):\n",
        "    if model == 'online':\n",
        "      return self.online(inputs)\n",
        "    elif model == 'target':\n",
        "      return self.target(inputs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srnenWg2cBUh"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, state_dim, action_dim, save_dir, checkpoint=False):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.memory = deque(maxlen=100000)\n",
        "    self.batch_size = 32\n",
        "\n",
        "    self.exploration_rate = 1\n",
        "    self.exploration_rate_decay = 0.9999995\n",
        "    self.exploration_rate_min = 0.1\n",
        "    self.gamma = 0.9\n",
        "\n",
        "    self.curr_step = 0\n",
        "    self.burnin = 1e5\n",
        "    self.learn_every = 3\n",
        "    self.sync_every = 1e4\n",
        "\n",
        "    self.save_every = 5e5\n",
        "    self.save_dir = save_dir\n",
        "\n",
        "    self.gpu = torch.cuda.is_available()\n",
        "\n",
        "    self.net = ArseNet(self.state_dim, self.action_dim).float()\n",
        "    if self.gpu:\n",
        "      self.net = self.net.to(device='cuda')\n",
        "    if checkpoint:\n",
        "      self.load(checkpoint)\n",
        "    \n",
        "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=3e-4)\n",
        "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "  \n",
        "  def act(self, state):\n",
        "    '''\n",
        "    choose epsilon-greedy action given state and update the value of step\n",
        "    '''\n",
        "    # exploration\n",
        "    if np.random.rand() < self.exploration_rate:\n",
        "      action_idx = np.random.randint(self.action_dim)\n",
        "    # exploitation\n",
        "    else:\n",
        "      state = torch.FloatTensor(state).cuda() if self.gpu else torch.FloatTensor(state)\n",
        "      state = state.unsqueeze(0)\n",
        "      action_values = self.net(state, model='online')\n",
        "      action_idx = torch.argmax(action_values, axis=1).item()\n",
        "    \n",
        "    # decrease exploration_rate\n",
        "    self.exploration_rate *= self.exploration_rate_decay\n",
        "    if self.exploration_rate < self.exploration_rate_min:\n",
        "      self.exploration_rate = self.exploration_rate_min\n",
        "    \n",
        "    self.curr_step += 1\n",
        "    return action_idx\n",
        "  \n",
        "  def cache(self, state, next_state, action, reward, done):\n",
        "    '''\n",
        "    store the experience to self.memory\n",
        "    '''\n",
        "    state = torch.FloatTensor(state).cuda() if self.gpu else torch.FloatTensor(state)\n",
        "    next_state = torch.FloatTensor(next_state).cuda() if self.gpu else torch.FloatTensor(next_state)\n",
        "    action = torch.LongTensor([action]).cuda() if self.gpu else torch.LongTensor([action])\n",
        "    reward = torch.DoubleTensor([reward]).cuda() if self.gpu else torch.DoubleTensor([reward])\n",
        "    done = torch.BoolTensor([done]).cuda() if self.gpu else torch.BoolTensor([done])\n",
        "\n",
        "    self.memory.append((state, next_state, action, reward, done,))\n",
        "  \n",
        "  def recall(self):\n",
        "    '''\n",
        "    Take a batch of experience from memory\n",
        "    '''\n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "  \n",
        "  def td_estimate(self, state, action):\n",
        "    # the predicted optimal Q_online for a given state\n",
        "    current_Q = self.net(state, model=\"online\")[np.arange(0, self.batch_size), action]\n",
        "    return current_Q\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def td_target(self, reward, next_state, done):\n",
        "    '''\n",
        "    TD Target - aggregation of current reward and the estimated Q\n",
        "    in the next state\n",
        "    '''\n",
        "    next_state_Q = self.net(state, model='online')\n",
        "    best_action = torch.argmax(next_state_Q, axis=1)\n",
        "    next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
        "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "  \n",
        "  def update_Q_online(self, td_estimate, td_target):\n",
        "    loss = self.loss_fn(td_estimate, td_target)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss.item()\n",
        "  \n",
        "  def sync_Q_target(self):\n",
        "    self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "  \n",
        "  def save(self):\n",
        "    num = int(self.curr_step // self.save_every)\n",
        "    save_path = (self.save_dir / f\"mario_net_{num}.chkpt\")\n",
        "    torch.save(dict(model=self.net.state_dict(),\n",
        "                    exploration_rate=self.exploration_rate), save_path)\n",
        "    print(f\"ArseNet saved to {save_path} at step {self.curr_step}\")\n",
        "  \n",
        "  def learn(self):\n",
        "    if not self.curr_step % self.sync_every:\n",
        "      self.sync_Q_target()\n",
        "\n",
        "    if not self.curr_step % self.save_every:\n",
        "      self.save()\n",
        "\n",
        "    if self.curr_step < self.burnin:\n",
        "      return None, None\n",
        "\n",
        "    if self.curr_step % self.learn_every:\n",
        "      return None, None\n",
        "\n",
        "    \n",
        "    # sample from memory\n",
        "    state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "    # TD Estimate and TD Target\n",
        "    td_est = self.td_estimate(state, action)\n",
        "    td_trgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "    loss = self.update_Q_online(td_est, td_trgt)\n",
        "\n",
        "    return td_est.mean().item(), loss\n",
        "  \n",
        "  def save(self):\n",
        "    save_path = self.save_dir / f\"arse_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "    torch.save(\n",
        "        dict(\n",
        "            model=self.net.state_dict(),\n",
        "            exploration_rate=self.exploration_rate\n",
        "        ),\n",
        "        save_path\n",
        "    )\n",
        "    print(f\"ArseNet saved to {save_path} at step {self.curr_step}\")\n",
        "  \n",
        "  def load(self, load_path):\n",
        "    if not load_path.exists():\n",
        "      raise ValueError(f\"{load_path} does not exist\")\n",
        "\n",
        "    ckp = torch.load(load_path, map_location=('cuda' if self.gpu else 'cpu'))\n",
        "    exploration_rate = ckp.get('exploration_rate')\n",
        "    state_dict = ckp.get('model')\n",
        "\n",
        "    print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
        "    self.net.load_state_dict(state_dict)\n",
        "    self.exploration_rate = exploration_rate"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZY3ZrJfrJZX"
      },
      "source": [
        "class MetricLogger():\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "wxXp_qXchUWm",
        "outputId": "98c0a208-6db6-4d1e-ea92-6213e6ed7ade"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Agent(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 10\n",
        "\n",
        "for e in range(episodes):\n",
        "  state = env.reset()\n",
        "\n",
        "  while True:\n",
        "    action = mario.act(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "    q, loss = mario.learn()\n",
        "    logger.log_step(reward, loss, q)\n",
        "    state = next_state\n",
        "    \n",
        "    if done or info['flag_get']:\n",
        "      break\n",
        "\n",
        "  logger.log_episode()\n",
        "\n",
        "  if e % 20 == 0:\n",
        "    logger.record(\n",
        "        episode=e,\n",
        "        epsilon=mario.exploration_rate,\n",
        "        step=mario.curr_step\n",
        "        )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 0 - Step 58 - Epsilon 0.9999710004132487 - Mean Reward 202.0 - Mean Length 58.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.723 - Time 2021-07-06T17:54:10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAbucsOxraXU"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNMghuaJrvPl"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7lghNoHr09E",
        "outputId": "0853c483-ad02-4e35-a1e1-0d19b54073a1"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f0d52b14510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH6B8L-tr5NZ"
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa_KdRByr5-7"
      },
      "source": [
        "env = wrap_env(env)\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LstGeKl8l1L"
      },
      "source": [
        "while not done:\n",
        "    env.render()\n",
        "    action = mario.act(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "env.close()\n",
        "print(total_reward)\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKA2z1Ch-ay2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}